---
layout: post
title: "Project KBCard- Issue"
author: "Bys"
category: issue
date: 2021-04-03 01:00:00
tags: troubleshooting issue
---

#### **- Secondary IP & DNS 이슈**

EKS Pod에서 Secondary IP를 사용하면서 DNS Lookup이 되지 않은 현상  
Pod의 /etc/resolv.conf 에서 172.20.0.10 의 nameserver 셋팅이 되어 있었음  
```bash
caused by: Error: RequestError: send request failed 
caused by: Post  dial tcp: i/o timeout
```

위 와 같은 오류가 발생하였으며 bootstarp.sh 파일을 살펴보면 아래의 내용이 존재한다.  
DNS_CLUSTER_IP를 kube-dns 서비스의 CLUSTER-IP(kubectl get svc kube-dns -n kube-system / 172.16.0.10)로 값을 넘겨주는 셋팅을 한다.  
해당 설정을 마치면 Pod의 nameserver가 core-dns의 nameserver로 정상 셋팅이 된다.  

```bash
while [[ $# -gt 0 ]]; do
    key="$1"
    case $key in
        -h|--help)
            print_help
            exit 1
            ;;
        --use-max-pods)
            USE_MAX_PODS="$2"
            shift
            shift
            ;;
        --b64-cluster-ca)
            B64_CLUSTER_CA=$2
            shift
            shift
            ;;
        --apiserver-endpoint)
            APISERVER_ENDPOINT=$2
            shift
            shift
            ;;
        --kubelet-extra-args)
            KUBELET_EXTRA_ARGS=$2
            shift
            shift
            ;;
        --enable-docker-bridge)
            ENABLE_DOCKER_BRIDGE=$2
            shift
            shift
            ;;
        --aws-api-retry-attempts)
            API_RETRY_ATTEMPTS=$2
            shift
            shift
            ;;
        --docker-config-json)
            DOCKER_CONFIG_JSON=$2
            shift
            shift
            ;;
        --pause-container-account)
            PAUSE_CONTAINER_ACCOUNT=$2
            shift
            shift
            ;;
        --pause-container-version)
            PAUSE_CONTAINER_VERSION=$2
            shift
            shift
            ;;
        --dns-cluster-ip)
            DNS_CLUSTER_IP=$2
            shift
            shift
            ;;
        *)    # unknown option
            POSITIONAL+=("$1") # save it in an array for later
            shift # past argument
            ;;
    esac
done

set +u
set -- "${POSITIONAL[@]}" # restore positional parameters
CLUSTER_NAME="$1"
set -u

USE_MAX_PODS="${USE_MAX_PODS:-true}"
B64_CLUSTER_CA="${B64_CLUSTER_CA:-}"
APISERVER_ENDPOINT="${APISERVER_ENDPOINT:-}"
SERVICE_IPV4_CIDR="${SERVICE_IPV4_CIDR:-}"
DNS_CLUSTER_IP="${DNS_CLUSTER_IP:-}"
KUBELET_EXTRA_ARGS="${KUBELET_EXTRA_ARGS:-}"
ENABLE_DOCKER_BRIDGE="${ENABLE_DOCKER_BRIDGE:-false}"
API_RETRY_ATTEMPTS="${API_RETRY_ATTEMPTS:-3}"
DOCKER_CONFIG_JSON="${DOCKER_CONFIG_JSON:-}"
PAUSE_CONTAINER_VERSION="${PAUSE_CONTAINER_VERSION:-3.1-eksbuild.1}"

......

if [[ -z "${DNS_CLUSTER_IP}" ]]; then
  if [[ ! -z "${SERVICE_IPV4_CIDR}" ]] && [[ "${SERVICE_IPV4_CIDR}" != "None" ]] ; then
    #Sets the DNS Cluster IP address that would be chosen from the serviceIpv4Cidr. (x.y.z.10)
    DNS_CLUSTER_IP=${SERVICE_IPV4_CIDR%.*}.10
  else
    MAC=$(get_meta_data 'latest/meta-data/network/interfaces/macs/' | head -n 1 | sed 's/\/$//')
    TEN_RANGE=$(get_meta_data "latest/meta-data/network/interfaces/macs/$MAC/vpc-ipv4-cidr-blocks" | grep -c '^10\..*' || true )
    DNS_CLUSTER_IP=10.100.0.10
    if [[ "$TEN_RANGE" != "0" ]]; then
      DNS_CLUSTER_IP=172.20.0.10
    fi
  fi
else
  DNS_CLUSTER_IP="${DNS_CLUSTER_IP}"
fi
```
<br><br>


#### **- aws-load-balancer-controller 및 Ingress 배포**

폐쇄망 환경에서 ALB Ingress 배포시 wafv2.ap-northeast-2.amazonaws.com 에서 Connection Timeout이 발생한 현상  
```bash
caused by: wafv2.ap-northeast-2.amazonaws.com Post  dial tcp: i/o timeout
```

aws-load-balancer-controller를 배포할 때 Helm 배포를 진행하면서 기본 Values 값에 아래와 같은 설정이 들어있었음(ALB Ingress 배포 참고)  
`Values.yaml`
```bash
# Enable Shield addon for ALB (default true)
enableShield:

# Enable WAF addon for ALB (default true)
enableWaf:

# Enable WAF V2 addon for ALB (default true)
enableWafv2:
```

따라서 폐쇄망 환경에서는 아래와 같이 Helm 배포를 진행하였음  

`aws-load-balancer-controller Helm 배포`
```bash
helm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller \
  --set clusterName=ekscluster-name \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set image.repository=222383050459.dkr.ecr.ap-northeast-2.amazonaws.com/opensource-components \
  --set image.tag=aws-load-balancer-controller-v2.2.0 \
  --set enableShield=false \
  --set enableWaf=false \
  --set enableWafv2=false \
  -n kube-system
```
<br><br>

#### **- ElasticSearch EBS Full**
Kibana에 어느 순간 부터 로그가 쌓이지 않았고, Cloudwatch에 Lambda 로그를 확인 한 결과 아래와 같이 로그가 발생하였음  
```yaml
ERROR Invoke Error
{
    "errorType": "Error",
    "errorMessage": "{\"statusCode\":200,\"responseBody\":{\"took\":0,\"errors\":true}}",
    "stack": [
        "Error: {\"statusCode\":200,\"responseBody\":{\"took\":0,\"errors\":true}}",
        "    at _homogeneousError (/var/runtime/CallbackContext.js:12:12)",
        "    at postError (/var/runtime/CallbackContext.js:29:51)",
        "    at done (/var/runtime/CallbackContext.js:56:7)",
        "    at fail (/var/runtime/CallbackContext.js:68:7)",
        "    at Object.fail (/var/runtime/CallbackContext.js:104:16)",
        "    at /var/task/index.js:43:25",
        "    at IncomingMessage.<anonymous> (/var/task/index.js:177:13)",
        "    at IncomingMessage.emit (events.js:203:15)",
        "    at endReadableNT (_stream_readable.js:1145:12)",
        "    at process._tickCallback (internal/process/next_tick.js:63:19)"
    ]
}
```
위 로그가 명확하지 않아 console.log 로 failedItems를 아래와 같이 추가로 찍어본 결과  
```javascript
post(elasticsearchBulkData, function(error, success, statusCode, failedItems) {
            // 수정완료
            console.log('Failed: ' + JSON.stringify(failedItems));
            console.log('Response: ' + JSON.stringify({ 
                "statusCode": statusCode 
            }));
```

아래와 같은 오류가 발생하였음  
```yaml
{
  "error": {
    "root_cause": [
      {
        "type": "index_create_block_exception",
        "reason": "blocked by: [FORBIDDEN/10/cluster create-index blocked (api)];"
      }
    ],
    "type": "index_create_block_exception",
    "reason": "blocked by: [FORBIDDEN/10/cluster create-index blocked (api)];"
  },
  "status": 403
```
이 오류는 ElasticSearch의 여유 스토리지 공간 부족하여 발생한 문제 였으며 EBS 스토리지 공간을 늘려 조치 완료 하였음 

<br><br>


#### **- ElasticSearch Sharding 부족**  
Kibana에 어느 순간 부터 로그가 쌓이지 않았고, Cloudwatch에 Lambda 로그를 확인 한 결과 아래와 같이 로그가 발생하였음 (failedItems 을 찍어서 확인할 것)  
Shards가 부족한 현상이 발생하고 있었고, 기존의 index를 삭제하면서 임시로 해결하였다.  
이 후 policy를 생성하여 과거 Index를 관리하는 정책을 만들어 적용하였다.  

```yaml
{
    "index": {
        "_index": "mydata-ubd-bill-api-dev-2021.06.28",
        "_type": "/aws/containerinsights/mydata-cluster-dev/application/ubd/bill-api-dev",
        "_id": "36235500043077433227889076052461137394947113222921781845",
        "status": 400,
        "error": {
            "type": "validation_exception",
            "reason": "Validation Failed: 1: this action would add [10] total shards, but this cluster currently has [999]/[1000] maximum shards open;"
        }
    }
}
```
<br><br>


#### **- Kibana 로그 섞임**
Kibana화면에서 조회 되는 로그가 섞여서 보이는 현상이 발생  
fluentD.yaml 파일의 configmap에서 multiline_start_regexp을 설정하여 로그를 묶었음  

Log 패턴을 정규 표현식에 맞추어 하나의 로그 그룹으로 묶어 Clodwatch로 전송  

-로그 패턴  
```bash
11:09:04[301] logdata~~~~~~
(T) 11:09:04[301] logdata~~~~~~
logdata~~~~~~
11:09:04[301] logdata~~~~
```

아래와 같이 multiline_start_regexp 수정  
```ruby
<label @containers>
......
<filter **>
  @type concat
  key log
  # 수정완료
  multiline_start_regexp /\((D|E|T|I)\)\s\d{2}[:]\d{2}[:]\d{2}\[\d{3}\]/
  #multiline_start_regexp /^(date:){0,1}\d{4}[-/]\d{1,2}[-/]\d{1,2}/
  separator ""
  flush_interval 5
  timeout_label @NORMAL
</filter>
```
<br><br>


#### **- wasdk 인증 모듈 이슈**  
인증모듈 라이브러리를 /fsutil/wasdk/jar 추가하였으나 테스트 시 java.lang.UnsatisfiedLinkError 오류 발생  
도커파일에서 기존애 아래와 같이 라이브러리 Path를 잡아 주고 있었고  
```Dockerfile
ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/fsutil/scp/ba_scp"
ENV JAVA_OPTS="${JAVA_OPTS}:/fsutil/scp/ba_scp"
```

아래와 같이 /fsutil/wasdk/jar 경로를 LD_LIBRARY_PATH 에 추가 해 주었음  
```Dockerfile
ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/fsutil/scp/ba_scp"
ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/fsutil/wasdk/jar"
ENV JAVA_OPTS="${JAVA_OPTS}:/fsutil/scp/ba_scp"
```
LD_LIBRARY_PATH 환경 변수에 등록을 해주면 `JAVA_OPTS = "-Djava.library.path=/fsutil/scp/bascp"` 를 주는 것과 동일하기 때문에 
아마도 마지막 변수가 설정되면서 /fsutil/wasdk/jar 경로가 제대로 인식되지 않은 것으로 보여 LD_LIBRARY_PATH와 JVM -D 옵션이 중복되지 않도록 아래와 같이 설정을 변경 하여 해결  
```Dockerfile
ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/fsutil/scp/ba_scp"
ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/fsutil/wasdk/jar"
```
<br><br>


#### **- coredns Configmap 변경**  
EKS Add-ons로 제공 되는 coredns 의 경우 configmap 수정이 불가하다.  
coredns configmap을 수정해서 사용하고 싶은 경우에는 Add-on에서 coredns를 제거하고  
별도로 coredns를 배포하여 변경 건을 배포하여야 한다.  

<br><br>



#### **- 성능테스트 - Node 통신 오류 및 OOM Killed**  
정상적으로 배포 후 성능테스트를 진행하였다.  
부하가 들어오기 시작하자 갑자기 Pod 들이 OOMKilled 오류와 함께 죽기 시작했고, 재 생성되는 Pod에서는 ContainerCreating 상태에서 넘어가지 않는 현상이 있었다.  
Pod를 Describe 해보면 Kubelet 과의 통신이 끊어진 현상이 보인다. 정확한 현상은 알 수 없지만 rpc error가 발생하고 그 당시 top으로 nodes의 리소스 상태를 보면
모두 정상적으로 보이던 Node의 리소스들이 unknown 상태로 변경 되었다.  

이 당시 OOMKilled 가 발생하던 원인을 찾기 위해 Thread Dump를 떴으며 해당 Thread Dump를 통해 Pod에서 부하를 받을 당시 3000개 이상의 Kafka관련 Thread를 생성하는 것으로 확인이 되었다.  
개발 소스에서 Producer.send()를 할 때마다 new Thread()를 생성하여 Thread가 지속생성 된 것으로 확인 되었으며 

Log Level을 debug, trace 레벨로 모두 찍고 있어 fluentD가 Cloudwatch로 해당 로그 스트림을 전송하면서 많은 부하가 걸렸던 것으로 보인다. 

결론적으로는, Pod에서 Thread를 무한정 생성하다가 WorkerNode에 까지 영향을 준 것으로 보였다.  

```cmd
[cicdadm@ppmacmd1 .kube]$ k get po -n ubd -o wide
NAME                                   READY   STATUS              RESTARTS   AGE   IP               NODE                                               NOMINATED NODE   READINESS GATES
appr-api-prd-deploy-6d565fdcfd-2pb8c   0/1     ContainerCreating   0          18m   <none>           ip-10-95-254-167.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6d565fdcfd-nk9rs   1/1     Running             0          18m   100.64.146.191   ip-10-95-255-190.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6d565fdcfd-wnvth   0/1     ContainerCreating   0          18m   <none>           ip-10-95-254-189.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6d565fdcfd-xmgmq   1/1     Running             0          18m   100.64.150.50    ip-10-95-255-173.ap-northeast-2.compute.internal   <none>           <none>
bill-api-prd-deploy-67b84cbff6-57bck   1/1     Running             0          64m   100.64.144.151   ip-10-95-255-181.ap-northeast-2.compute.internal   <none>           <none>
main-api-prd-deploy-86d8979884-zdgjm   1/1     Running             0          63m   100.64.133.231   ip-10-95-254-167.ap-northeast-2.compute.internal   <none>           <none>

[cicdadm@ppmacmd1 stable]$ k describe po appr-api-prd-deploy-7857fc7545-g24dg -n ubd
Warning  FailedCreatePodSandBox  18m                   kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod "appr-api-prd-deploy-7857fc7545-g24dg": operation timeout: context deadline exceeded
  Warning  FailedSync              3m29s (x47 over 16m)  kubelet            error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded

[cicdadm@ppmacmd1 .kube]$ k top nodes
NAME                                               CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-95-254-184.ap-northeast-2.compute.internal   219m         2%     18109Mi         59%
ip-10-95-254-186.ap-northeast-2.compute.internal   170m         2%     4806Mi          15%
ip-10-95-255-167.ap-northeast-2.compute.internal   223m         2%     14591Mi         48%
ip-10-95-255-173.ap-northeast-2.compute.internal   139m         1%     3891Mi          12%
ip-10-95-255-181.ap-northeast-2.compute.internal   165m         2%     4301Mi          14%
ip-10-95-255-190.ap-northeast-2.compute.internal   107m         1%     2530Mi          8%
ip-10-95-254-167.ap-northeast-2.compute.internal   <unknown>                           <unknown>               <unknown>               <unknown>
ip-10-95-254-189.ap-northeast-2.compute.internal   <unknown>                           <unknown>               <unknown>               <unknown>

[cicdadm@ppmacmd1 .kube]$ k logs -f metrics-server-76c66fffdb-56srj -n kube-system
E0719 02:44:30.160694       1 server.go:132] unable to fully scrape metrics: unable to fully scrape metrics from node ip-10-95-254-188.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-254-188.ap-northeast-2.compute.internal: Get "https://10.95.254.188:10250/stats/summary?only_cpu_and_memory=true": dial tcp 10.95.254.188:10250: connect: connection refused
E0721 02:42:00.180656       1 server.go:132] unable to fully scrape metrics: [unable to fully scrape metrics from node ip-10-95-254-164.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-254-164.ap-northeast-2.compute.internal: Get "https://10.95.254.164:10250/stats/summary?only_cpu_and_memory=true": dial tcp 10.95.254.164:10250: i/o timeout, unable to fully scrape metrics from node ip-10-95-254-166.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-254-166.ap-northeast-2.compute.internal: Get "https://10.95.254.166:10250/stats/summary?only_cpu_and_memory=true": dial tcp 10.95.254.166:10250: i/o timeout]
E0721 02:45:00.138609       1 server.go:132] unable to fully scrape metrics: unable to fully scrape metrics from node ip-10-95-255-187.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-255-187.ap-northeast-2.compute.internal: Get "https://10.95.255.187:10250/stats/summary?only_cpu_and_memory=true": dial tcp 10.95.255.187:10250: i/o timeout
E0721 02:54:48.179206       1 server.go:132] unable to fully scrape metrics: unable to fully scrape metrics from node ip-10-95-255-177.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-255-177.ap-northeast-2.compute.internal: Get "https://10.95.255.177:10250/stats/summary?only_cpu_and_memory=true": dial tcp 10.95.255.177:10250: connect: no route to host
E0721 09:49:24.117567       1 server.go:132] unable to fully scrape metrics: [unable to fully scrape metrics from node ip-10-95-254-167.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-254-167.ap-northeast-2.compute.internal: Get "https://10.95.254.167:10250/stats/summary?only_cpu_and_memory=true": context deadline exceeded, unable to fully scrape metrics from node ip-10-95-254-189.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-254-189.ap-northeast-2.compute.internal: Get "https://10.95.254.189:10250/stats/summary?only_cpu_and_memory=true": context deadline exceeded]
```

<br><br>


#### **- 성능테스트 - Pod Scale-In 메모리 조건**  

Pod의 Request, Limit은 모두 동일한 리소스를 갖도록 설정하였으며 cpu 1100m, memory 3Gi 로 설정이 되어있었고, 
Tomcat Process의 Heap 메모리설정은 -Xms 2048m, -Xmx 2048m -XX:MetaspaceSize 512m -XX:MaxMetaspaceSize 512m 으로 설정하였다.  
HPA조건은 CPU 60%, Memory 70%의 조건이었으며 Scale-Out은 모두 정상적으로 되었으나 Scale-In이 정상적으로 되지 않는 현상이 있었다.  

메모리 초기 사용량이 1Gi에서 2.7Gi 까지 늘어나면서 Scale-in 조건이 맞지 않는 현상이었는데  
추측으로는 HeapMemory로 2048m 을 초기부터 전체 물리메모리로 할당하여 사용하지 않는 것으로보이고 점진적으로 사용하다가 늘린 이 후에 할당된 메모리만큼을 쭉 사용하는 것으로 보였다.  

따라서, 메모리 사용량은 실제 hpa 조건에서 제거하였다.  
이 부분은 추가적인 확인이 필요해보인다.  

```cmd
[cicdadm@ppmacmd1 alb-ingress]$ k get pod -n ubd -o wide
NAME                                   READY   STATUS    RESTARTS   AGE     IP               NODE                                               NOMINATED NODE   READINESS GATES
appr-api-prd-deploy-6974c449d9-5m9br   1/1     Running   0          2d13h   100.64.132.232   ip-10-95-254-183.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-5nwck   0/1     Running   0          2d12h   100.64.132.87    ip-10-95-254-180.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-5qcxr   1/1     Running   0          2d13h   100.64.147.8     ip-10-95-255-176.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-7kq5h   1/1     Running   0          2d12h   100.64.148.209   ip-10-95-255-183.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-8knnm   1/1     Running   0          2d12h   100.64.130.10    ip-10-95-254-180.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-9z7pl   1/1     Running   0          2d13h   100.64.151.62    ip-10-95-255-183.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-ccmgs   1/1     Running   0          2d13h   100.64.132.61    ip-10-95-254-167.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-dl5zz   1/1     Running   0          2d12h   100.64.130.11    ip-10-95-254-167.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-dv66g   1/1     Running   0          2d13h   100.64.129.10    ip-10-95-254-186.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-h2m28   1/1     Running   0          2d14h   100.64.146.191   ip-10-95-255-190.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-lfgh9   1/1     Running   0          2d12h   100.64.149.150   ip-10-95-255-176.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-nrd6n   1/1     Running   0          2d12h   100.64.130.222   ip-10-95-254-186.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-pmn4d   1/1     Running   0          2d14h   100.64.134.106   ip-10-95-254-183.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-st2lq   1/1     Running   0          2d13h   100.64.134.172   ip-10-95-254-183.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-t8z2k   1/1     Running   0          2d13h   100.64.131.13    ip-10-95-254-167.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-tf8cn   1/1     Running   0          2d13h   100.64.151.236   ip-10-95-255-176.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-tm4mk   1/1     Running   0          2d12h   100.64.149.36    ip-10-95-255-190.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-wtfl5   1/1     Running   0          2d13h   100.64.149.151   ip-10-95-255-190.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-xvn99   1/1     Running   0          2d14h   100.64.131.40    ip-10-95-254-186.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-zgz8w   1/1     Running   0          2d14h   100.64.149.61    ip-10-95-255-183.ap-northeast-2.compute.internal   <none>           <none>

#[cicdadm@ppmacmd1 alb-ingress]$ k top pod -n ubd
#NAME                                   CPU(cores)   MEMORY(bytes)
#appr-api-prd-deploy-6974c449d9-5m9br   48m          1000Mi


[cicdadm@ppmacmd1 alb-ingress]$ k top pod -n ubd
NAME                                   CPU(cores)   MEMORY(bytes)
appr-api-prd-deploy-6974c449d9-5m9br   48m          2573Mi
appr-api-prd-deploy-6974c449d9-5nwck   85m          2146Mi
appr-api-prd-deploy-6974c449d9-5qcxr   59m          2536Mi
appr-api-prd-deploy-6974c449d9-7kq5h   43m          2554Mi
appr-api-prd-deploy-6974c449d9-8knnm   63m          2541Mi
appr-api-prd-deploy-6974c449d9-9z7pl   46m          2548Mi
appr-api-prd-deploy-6974c449d9-ccmgs   36m          2567Mi
appr-api-prd-deploy-6974c449d9-dl5zz   66m          2522Mi
appr-api-prd-deploy-6974c449d9-dv66g   45m          2552Mi
appr-api-prd-deploy-6974c449d9-h2m28   38m          2555Mi
appr-api-prd-deploy-6974c449d9-lfgh9   53m          2554Mi
appr-api-prd-deploy-6974c449d9-nrd6n   39m          2533Mi
appr-api-prd-deploy-6974c449d9-pmn4d   45m          2579Mi
appr-api-prd-deploy-6974c449d9-st2lq   48m          2565Mi
appr-api-prd-deploy-6974c449d9-t8z2k   57m          2548Mi
appr-api-prd-deploy-6974c449d9-tf8cn   56m          2575Mi
appr-api-prd-deploy-6974c449d9-tm4mk   42m          2527Mi
appr-api-prd-deploy-6974c449d9-wtfl5   57m          2562Mi
appr-api-prd-deploy-6974c449d9-xvn99   47m          2557Mi
appr-api-prd-deploy-6974c449d9-zgz8w   44m          2554Mi
```
<br><br>



#### **- 성능테스트 - ThreadDump**  

Thread Dump는 JVM에서 발생하는 이슈에 대해서 가장 디테일하게 볼 수 있는 정보라고 생각한다.  

실제로 마이데이터 성능테스트 당시 발생했던 모든 이슈에 대해서 Thread Dump를 통해 해결하였으며  
문제가 발생하는 시점에 Tomcat 프로세스 내부에서 Thread들이 어떤 상태를 가지고 어떤 작업을 하고 있었는지를 알 수 있었다.  

Linux에서는 Thread Dump를 kill -3 process_id 를 이용하여 Dump를 뜰 수 있었다.  

Pod에서는 콘솔로그로 출력이 되어 아래와 같은 커맨드를 통해 Thread Dump를 생성했다.  
```bash
kubectl logs -f pod_name >> pod_thread.log 
#kubectl exec -it pod_name -n namespace /bin/bash 
#kill -3 1 
```

실제로 보고 싶은 시점에서 Thread Dump를 3~4번 정도 생성하였으며 많게는 10번도 넘게 Thread Dump를 생성하였다.  
그리고 문제가 발생했던 시점에서 5~10초 정도가 흐른 후 한 번더 Thread Dump를 생성하였고 다양한 시점에서 분석을 하기 위한 Thread Dump를 생성했다.  

