---
layout: post
title: "Project - KBCard 성능테스트"
author: "Bys"
category: project
date: 2021-04-03 01:00:00
tags: project issue
---

#### **- 성능테스트**  

성능 테스트는 시간당 사용자수, 분당 사용자수, 초당 사용자수, 사용자당 트랜잭션 수 등을 통해 초당 트랜잭션 수를  계산한다.  
이 번 성능테스트의 목표 TPS는 2100 TPS로 목표 응답시간은 2초 이내 응답이다.  

테스트 시나리오는 3개의 시나리오로 진행한다.  
1. 목표 성능 테스트 - 복합 업무를 대상으로 산정된 목표 부하를 부여하였을 때, 성능 목표에 도달 하는지 여부를 확인  
대상업무: API 4개  //  부하발생: 5분간 Ramp Up  //  부하지속: 10분간 지속

![k01](/assets/it/project/kbcard/mydata/k01.png){: width="50%" height="auto"}  

2. 임계부하 성능 테스트 - 복합 업무를 대상으로 최대 부하를 부여하였을 때, 처리 가능한 최대 TPS 수준을 확인
대상업무: API 4개  //  부하발생: 1초당 1명씩 Ramp Up  //  부하지속: 최대 TPS 도달 시 종료  

![k02](/assets/it/project/kbcard/mydata/k02.png){: width="50%" height="auto"}  

3. 장시간 안정성 테스트 - 목표 부하 성능테스트를 장시간(최소6시간) 수행하여 시스템의 장시간 부하 영향을 확인  
대상업무: API 4개  //  부하지속: 6시간 ~ 24시간  

![k03](/assets/it/project/kbcard/mydata/k03.png){: width="50%" height="auto"}  

<br><br>


#### **- 성능테스트 - UBF 튜닝1**  

초기 4개의 Pod에서 부하를 시작하였을 때, 응답시간 지연이 쌓이기 시작하였다.  

![k04](/assets/it/project/kbcard/mydata/k04.png){: width="100%" height="auto"}  

Jennifer 상에서 증가하는 응답시간을 잡아보니 실제 SQL 수행시간이 오래걸렸고 해당 Query에 대한 튜닝을 진행하고 나서야 정상 응답시간으로 복구하였다.  

![k05](/assets/it/project/kbcard/mydata/k05.jpg){: width="100%" height="auto"}  

<br><br>



#### **- 성능테스트 - UBF 튜닝2**  

초기 4개의 Pod에서 부하를 시작하였을 때, Scale-Out이 되면서 갑자기 모든 Pod에 부하가 걸리면서 죽는 현상이 발생하였다.  
Scale-Out이 되었지만 Scale-Out 된 Pod로 아직은 부하가 들어오기 전이라는 시점이 이상하여 Scale-Out이 문제였는지, 아니면 다른게 이상했는지 판단하기가 어려웠다.  
그래서 다시 한 번 테스트 수행을 요청하였고, 이번에는 Scale-Out이 되기 전에 죽는현상이 있었다.  

따라서 기존 소스코드에 무엇이 문제였는지 파악이 시급했다.  
다시 한 번 Test를 돌리면서 죽을 때 쯔음 Thread Dump를 생성하기 시작했다.  

![k06](/assets/it/project/kbcard/mydata/k06.png){: width="100%" height="auto"}  

제니퍼 상에서는 DB Connection get failed 오류가 많이 발생하였고 (Timeout 30초), ThreadDump를 보니 JDBC Getconnection을 waiting 하는 Thread들이 많이 있었다.  

정확한 이유는 모르겠지만 Main Thread에서 Nested transaction으로 별도 Thread를 생성하여 로그를 추가로 Insert 하는 로직이 있었는데 해당 로직을 빼고 수행해보니  
Pod이 죽지 않고 정상적으로 수행하는 것을 확인하였다.  

Http요청하나가 종료할 때 까지 DB Connection Pool하나를 사용한다고하여 (추가확인필요) 모든 Connection Pool이 Http 요청을 처리하는 중  
Nested Transaction 으로 분기된 Thread에서 Getconnection을 할 때 오류가 발생하나 싶어, Nested Transcation용 Connection Pool을 별도로 생성하였으며  
소스 상에서 로그 Insert하는 Nested Transaction은 해당 Connection Pool에서 Connection을 가져다가 사용하도록 소스 변경을 하였고 정상 처리되었다.  

조금 더 정확한 사유 확인이 필요한 케이스이다.  

```bash
at sun.misc.Unsafe.park(Native Method) 
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215) 
at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037) 
at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328) 
at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277) 
at org.apache.tomcat.jdbc.pool.FairBlockingQueue.poll(FairBlockingQueue.java:153) 
at org.apache.tomcat.jdbc.pool.ConnectionPool.borrowConnection(ConnectionPool.java:686) 
at org.apache.tomcat.jdbc.pool.ConnectionPool.getConnection(ConnectionPool.java:198) 
at org.apache.tomcat.jdbc.pool.DataSourceProxy.getConnection(DataSourceProxy.java:132) 
at aries.runtime.tracer.JDBCConnectionTrace.getConnection(JDBCConnectionTrace.java:123) 
at aries.runtime.tracer.impl.ProfileSQLConnectionImpl.getConnection(ProfileSQLConnectionImpl.java:371) 
at aries.base.profile.ProfileSQL.getConnection(ProfileSQL.java:325) 
at aries.base.jdk.DataSource.getConnection(DataSource.java:76) 
at devonframework.persistent.connection.LJndiDataSource.getConnection(LJndiDataSource.java:130) 
at devonframework.persistent.connection.LDataSourcePool.getJNDIConnection(LDataSourcePool.java:342) 
at devonframework.persistent.connection.LDataSourcePool.getConnection(LDataSourcePool.java:137) 
at devonframework.persistent.dao.LConnectionManager.getConnection(LConnectionManager.java:86) 
at devonframework.business.transaction.LConnectionMapper.getConnection(LConnectionMapper.java:106) 
at devonframework.persistent.autodao.LAutoDao.getConnection(LAutoDao.java:239) 
at devonframework.persistent.autodao.LAutoDao.getConnection(LAutoDao.java:213) 
at devonframework.persistent.autodao.LCommonDao.executeQueryForSingle(LCommonDao.java:790) 
at devonframework.persistent.autodao.LCommonDao.executeQueryForSingle(LCommonDao.java:494) 
at devonenterprise.service.tranctrl.composite.CompositeTranCtrl.checkCompositeSvcCtrl(CompositeTranCtrl.java:86) 
at devonenterprise.ext.front.command.RestInboundCommand.execute(RestInboundCommand.java:307) 
at devonenterprise.ext.channel.interceptor.ExtCommandInterceptor.executeCommand(ExtCommandInterceptor.java:116) 
at devonenterprise.ext.channel.interceptor.ExtCommandInterceptor.execute(ExtCommandInterceptor.java:87) 
at devonenterprise.ext.channel.interceptor.ExtCommandInterceptor.doIntercept(ExtCommandInterceptor.java:45) 
at devonframework.front.channel.interceptor.LInterceptorChain.doIntercept(LInterceptorChain.java:93) 
at devonenterprise.ext.channel.interceptor.GuidCreateInterceptor.doIntercept(GuidCreateInterceptor.java:27) 
at devonframework.front.channel.interceptor.LInterceptorChain.doIntercept(LInterceptorChain.java:93) 
at devonframework.front.channel.interceptor.LConverterInterceptor.doIntercept(LConverterInterceptor.java:171) 
at devonframework.front.channel.interceptor.LInterceptorChain.doIntercept(LInterceptorChain.java:93) 
at devonframework.front.channel.LAbstractServlet.process(LAbstractServlet.java:290) 
at devonenterprise.ext.channel.adaptor.http.sync.impl.GeneralRestServlet.catchService(GeneralRestServlet.java:62) 
at devonframework.front.channel.LAbstractServlet.doCommon(LAbstractServlet.java:167) 
at devonframework.front.channel.LAbstractServlet.doPost(LAbstractServlet.java:106) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:652) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:733) 
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) 
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
```
<br><br>


#### **- 성능테스트 - UBF 튜닝3**  

위 의 이슈들이 해결되고, 정상적으로 부하를 받던 중 Scale-Out된 Pod이 정상적으로 Heatlh Check된 이 후 문제가 발생하였다.  
ALB Target에 등록되어 트래픽을 받을 때 응답시간의 지연이 발생하는 경우였다.  
Jennifer 상에서는 Queue에 Tomcat Max Thread 개수인 300개가 꽉차서 처리가 되는 것 처럼 보였고, 응답시간이 7초정도까지 찬 상태로 면을 그리듯이 응답시간이 생겼다.  

Auto Scale-Out 된 Pod에 부하가 천천히 들어왔으면 좋겠다는 생각을 했고, Target Group Attributes 속성의 Slow start duration 옵션을 확인하여 설정을 진행했다.  

> Slow start duration  
During this period, a newly registered target receives an increasing share of requests, until it reaches its fair share.
Requires 30 to 900 seconds to enable, or 0 seconds to disable. This attribute cannot be combined with the Least outstanding requests algorithm.

초기 설정은 120초로 진행했다가 60초로 변경하였으며 해당 값 설정 이 후 확연히 눈에띄게 Scale-Out 서비스가 안정적인 트래픽을 받기 시작했다.  
Scale-Out 시에도 응답의 지연은 전혀 없었으며 안정적인 처리가 모두 가능했다.  

<br><br>


#### **- 성능테스트 - UBF 튜닝4**  

장기부하 테스트를 진행 중 항상 8시간 ~ 9시간 정도의 성능테스트 후 어플리케이션에서 응답시간이 지연되는 현상이 생겼다.  
계속 유지가 되던 Pod의 수도 2개 정도 Scale-Out이 되는 현상이 발생했다. 이 현상은 장기부하테스트를 2~3번 진행하는 동안 매번 동일한 패턴으로 8시간 ~ 9시간 사이에 발생했다.  

아래 빨간색 박스를 친 부분이 바로 응답시간의 지연이 생기기 시작한 패턴이며 신규로 생성된 2개의 파드에서는 현상이 없었고, 기존 4개의 파드에서만 현상이 발생했다.  

![k09](/assets/it/project/kbcard/mydata/k09.png){: width="100%" height="auto"}  

<br>

현상이 이상하여 리소스 상태를 모니터링 해봤다. 그리고 이상한 점을 발견했다. 메모리의 상태를 모니터링 해보니 아래와 같은 현상이 발생하고 있었다.  
4초 이상 걸리는 GC들이 수 없이 많이 발생하고 있었다.  그리고 Old 영역의 메모리들이 전혀 줄어들고 있는 패턴을 보이지 않았다.   

![k11](/assets/it/project/kbcard/mydata/k11.png){: width="100%" height="auto"}  

<br>

조금 더 자세하게 보기 위해 Pod 내부로 접속을 했다. 
jstat 커맨드로 상황을 보니 5초에 한 번씩 status를 체크했는데 Full GC Count가 계속 올라가고 있었고, Old Used 메모리는 떨어지지 않았다.  
때문에 메모리 누수를 확신하고 HeapDump를 생성하여 분석하기로 하였다.  

![k12](/assets/it/project/kbcard/mydata/k12.png){: width="100%" height="auto"}  

`Command`
```bash
jstat -gc 1 5000 100
```

<br>

HeapDump를 생성하고 MAT(Memory Analyzer Tool)를 이용해 파일을 열었다. 때문에 메모리 누수를 확신하고 HeapDump를 생성하여 분석하기로 하였다.  

![k13](/assets/it/project/kbcard/mydata/k13.png){: width="100%" height="auto"}  

Overview만 봐도 벌써 어디선가 1GB의 Heap영역을 차지하고 있음을 알 수 있다. 그 프로그램이 무엇인지를 찾기 위해 Dominator_tree를 열었다.

<br>

Retained_Heap 영역이 큰 것부터 찾아나가기 시작했다. 확률적으로 Retained_Heap이 큰 것 부터 찾으면 비정상적으로 금방 찾을 확률이 높다.  
Retained_Heap 메모리가 큰 것은 누수가 있기 때문에 지속적으로 참조메모리를 생성하고 있기 때문에 얼추 맞다.  

![k14](/assets/it/project/kbcard/mydata/k14.png){: width="100%" height="auto"}  

모든 Thread의 참조 메모리가 컸다. http-nio-11010-exec-2 번 Thread를 열어보기 시작했다.  

<br>

![k15](/assets/it/project/kbcard/mydata/k15.png){: width="100%" height="auto"}  

java.lang.Object 하위로 MapHashMap이 어디선가 5만개 가까이 생성 된 후 남아있었다.  

<br>

![k16](/assets/it/project/kbcard/mydata/k16.png){: width="100%" height="auto"}  

Framework 어디선가 Trace.log에 대한 HashMap을 생성하고 지속적으로 참조를 하고 있었기 때문에 HashMap 객체가 생성 된 후, 메모리 해제가 안되었던 현상이었다.   

<br>


![k17](/assets/it/project/kbcard/mydata/k17.png){: width="100%" height="auto"}  

Framework에서 트랜잭션이 끝난 후, resetAll() 호출을 통해 HashMap 변수에 null 처리를 해주도록 했으며 값이 없다보니 GC 시에 정상 메모리 해제가 된 듯 보였다. 
조치 후에 생긴 메모리는 위와 같이 변했다.  

<br>