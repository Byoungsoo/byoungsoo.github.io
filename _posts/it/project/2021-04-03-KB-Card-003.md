---
layout: post
title: "Project - KBCard 성능테스트"
author: "Bys"
category: project
date: 2021-04-03 01:00:00
tags: project issue
---

#### **- 성능테스트**  

성능 테스트는 시간당 사용자수, 분당 사용자수, 초당 사용자수, 사용자당 트랜잭션 수 등을 통해 초당 트랜잭션 수를  계산한다.  
이 번 성능테스트의 목표 TPS는 2100 TPS로 목표 응답시간은 2초 이내 응답이다.  

테스트 시나리오는 3개의 시나리오로 진행한다.  
1. 목표 성능 테스트 - 복합 업무를 대상으로 산정된 목표 부하를 부여하였을 때, 성능 목표에 도달 하는지 여부를 확인  
대상업무: API 4개  //  부하발생: 5분간 Ramp Up  //  부하지속: 10분간 지속

![k01](/assets/it/project/kbcard/mydata/k01.png){: width="50%" height="auto"}  

2. 임계부하 성능 테스트 - 복합 업무를 대상으로 최대 부하를 부여하였을 때, 처리 가능한 최대 TPS 수준을 확인
대상업무: API 4개  //  부하발생: 1초당 1명씩 Ramp Up  //  부하지속: 최대 TPS 도달 시 종료  

![k02](/assets/it/project/kbcard/mydata/k02.png){: width="50%" height="auto"}  

3. 장시간 안정성 테스트 - 목표 부하 성능테스트를 장시간(최소6시간) 수행하여 시스템의 장시간 부하 영향을 확인  
대상업무: API 4개  //  부하지속: 6시간 ~ 24시간  

![k03](/assets/it/project/kbcard/mydata/k03.png){: width="50%" height="auto"}  

<br><br>



#### **- 성능테스트 - UBD1 Node 통신 오류 및 OOM Killed**  
정상적으로 배포 후 성능테스트를 진행하였다.  
부하가 들어오기 시작하자 갑자기 Pod 들이 OOMKilled 오류와 함께 죽기 시작했고, 재 생성되는 Pod에서는 ContainerCreating 상태에서 넘어가지 않는 현상이 있었다.  
Pod를 Describe 해보면 Kubelet 과의 통신이 끊어진 현상이 보인다. 정확한 현상은 알 수 없지만 rpc error가 발생하고 그 당시 top으로 nodes의 리소스 상태를 보면
모두 정상적으로 보이던 Node의 리소스들이 unknown 상태로 변경 되었다.  

이 당시 OOMKilled 가 발생하던 원인을 찾기 위해 Thread Dump를 떴으며 해당 Thread Dump를 통해 Pod에서 부하를 받을 당시 3000개 이상의 Kafka관련 Thread를 생성하는 것으로 확인이 되었다.  
개발 소스에서 Producer.send()를 할 때마다 new Thread()를 생성하여 Thread가 지속생성 된 것으로 확인 되었다. 결론적으로는, Pod에서 Thread를 무한정 생성하다가 WorkerNode에 까지 영향을 준 것으로 보였다.  

추가적으로는 Log Level을 debug, trace 레벨로 모두 찍고 있어 fluentD가 Cloudwatch로 해당 로그 스트림을 전송하면서 많은 부하가 걸렸던 것으로 보인다. 


```cmd
[cicdadm@ppmacmd1 .kube]$ k get po -n ubd -o wide
NAME                                   READY   STATUS              RESTARTS   AGE   IP               NODE                                               NOMINATED NODE   READINESS GATES
appr-api-prd-deploy-6d565fdcfd-2pb8c   0/1     ContainerCreating   0          18m   <none>           ip-10-95-254-167.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6d565fdcfd-nk9rs   1/1     Running             0          18m   100.64.146.191   ip-10-95-255-190.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6d565fdcfd-wnvth   0/1     ContainerCreating   0          18m   <none>           ip-10-95-254-189.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6d565fdcfd-xmgmq   1/1     Running             0          18m   100.64.150.50    ip-10-95-255-173.ap-northeast-2.compute.internal   <none>           <none>
bill-api-prd-deploy-67b84cbff6-57bck   1/1     Running             0          64m   100.64.144.151   ip-10-95-255-181.ap-northeast-2.compute.internal   <none>           <none>
main-api-prd-deploy-86d8979884-zdgjm   1/1     Running             0          63m   100.64.133.231   ip-10-95-254-167.ap-northeast-2.compute.internal   <none>           <none>

[cicdadm@ppmacmd1 stable]$ k describe po appr-api-prd-deploy-7857fc7545-g24dg -n ubd
Warning  FailedCreatePodSandBox  18m                   kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod "appr-api-prd-deploy-7857fc7545-g24dg": operation timeout: context deadline exceeded
  Warning  FailedSync              3m29s (x47 over 16m)  kubelet            error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded

[cicdadm@ppmacmd1 .kube]$ k top nodes
NAME                                               CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
ip-10-95-254-184.ap-northeast-2.compute.internal   219m         2%     18109Mi         59%
ip-10-95-254-186.ap-northeast-2.compute.internal   170m         2%     4806Mi          15%
ip-10-95-255-167.ap-northeast-2.compute.internal   223m         2%     14591Mi         48%
ip-10-95-255-173.ap-northeast-2.compute.internal   139m         1%     3891Mi          12%
ip-10-95-255-181.ap-northeast-2.compute.internal   165m         2%     4301Mi          14%
ip-10-95-255-190.ap-northeast-2.compute.internal   107m         1%     2530Mi          8%
ip-10-95-254-167.ap-northeast-2.compute.internal   <unknown>                           <unknown>               <unknown>               <unknown>
ip-10-95-254-189.ap-northeast-2.compute.internal   <unknown>                           <unknown>               <unknown>               <unknown>

[cicdadm@ppmacmd1 .kube]$ k logs -f metrics-server-76c66fffdb-56srj -n kube-system
E0719 02:44:30.160694       1 server.go:132] unable to fully scrape metrics: unable to fully scrape metrics from node ip-10-95-254-188.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-254-188.ap-northeast-2.compute.internal: Get "https://10.95.254.188:10250/stats/summary?only_cpu_and_memory=true": dial tcp 10.95.254.188:10250: connect: connection refused
E0721 02:42:00.180656       1 server.go:132] unable to fully scrape metrics: [unable to fully scrape metrics from node ip-10-95-254-164.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-254-164.ap-northeast-2.compute.internal: Get "https://10.95.254.164:10250/stats/summary?only_cpu_and_memory=true": dial tcp 10.95.254.164:10250: i/o timeout, unable to fully scrape metrics from node ip-10-95-254-166.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-254-166.ap-northeast-2.compute.internal: Get "https://10.95.254.166:10250/stats/summary?only_cpu_and_memory=true": dial tcp 10.95.254.166:10250: i/o timeout]
E0721 02:45:00.138609       1 server.go:132] unable to fully scrape metrics: unable to fully scrape metrics from node ip-10-95-255-187.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-255-187.ap-northeast-2.compute.internal: Get "https://10.95.255.187:10250/stats/summary?only_cpu_and_memory=true": dial tcp 10.95.255.187:10250: i/o timeout
E0721 02:54:48.179206       1 server.go:132] unable to fully scrape metrics: unable to fully scrape metrics from node ip-10-95-255-177.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-255-177.ap-northeast-2.compute.internal: Get "https://10.95.255.177:10250/stats/summary?only_cpu_and_memory=true": dial tcp 10.95.255.177:10250: connect: no route to host
E0721 09:49:24.117567       1 server.go:132] unable to fully scrape metrics: [unable to fully scrape metrics from node ip-10-95-254-167.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-254-167.ap-northeast-2.compute.internal: Get "https://10.95.254.167:10250/stats/summary?only_cpu_and_memory=true": context deadline exceeded, unable to fully scrape metrics from node ip-10-95-254-189.ap-northeast-2.compute.internal: unable to fetch metrics from node ip-10-95-254-189.ap-northeast-2.compute.internal: Get "https://10.95.254.189:10250/stats/summary?only_cpu_and_memory=true": context deadline exceeded]
```

<br><br>


#### **- 성능테스트 - UBD2 Pod Scale-In 메모리 조건**  

Pod의 Request, Limit은 모두 동일한 리소스를 갖도록 설정하였으며 cpu 1100m, memory 3Gi 로 설정이 되어있었고, 
Tomcat Process의 Heap 메모리설정은 -Xms 2048m, -Xmx 2048m -XX:MetaspaceSize 512m -XX:MaxMetaspaceSize 512m 으로 설정하였다.  
HPA조건은 CPU 60%, Memory 70%의 조건이었으며 Scale-Out은 모두 정상적으로 되었으나 Scale-In이 정상적으로 되지 않는 현상이 있었다.  

메모리 초기 사용량이 1Gi에서 2.7Gi 까지 늘어나면서 Scale-in 조건이 맞지 않는 현상이었는데  
추측으로는 HeapMemory로 2048m 을 초기부터 전체 물리메모리로 할당하여 사용하지 않는 것으로보이고 점진적으로 사용하다가 늘린 이 후에 할당된 메모리만큼을 쭉 사용하는 것으로 보였다.  

따라서, 메모리 사용량은 실제 hpa 조건에서 제거하였다.  
이 부분은 추가적인 확인이 필요해보인다.  

```cmd
[cicdadm@ppmacmd1 alb-ingress]$ k get pod -n ubd -o wide
NAME                                   READY   STATUS    RESTARTS   AGE     IP               NODE                                               NOMINATED NODE   READINESS GATES
appr-api-prd-deploy-6974c449d9-5m9br   1/1     Running   0          2d13h   100.64.132.232   ip-10-95-254-183.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-5nwck   0/1     Running   0          2d12h   100.64.132.87    ip-10-95-254-180.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-5qcxr   1/1     Running   0          2d13h   100.64.147.8     ip-10-95-255-176.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-7kq5h   1/1     Running   0          2d12h   100.64.148.209   ip-10-95-255-183.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-8knnm   1/1     Running   0          2d12h   100.64.130.10    ip-10-95-254-180.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-9z7pl   1/1     Running   0          2d13h   100.64.151.62    ip-10-95-255-183.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-ccmgs   1/1     Running   0          2d13h   100.64.132.61    ip-10-95-254-167.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-dl5zz   1/1     Running   0          2d12h   100.64.130.11    ip-10-95-254-167.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-dv66g   1/1     Running   0          2d13h   100.64.129.10    ip-10-95-254-186.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-h2m28   1/1     Running   0          2d14h   100.64.146.191   ip-10-95-255-190.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-lfgh9   1/1     Running   0          2d12h   100.64.149.150   ip-10-95-255-176.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-nrd6n   1/1     Running   0          2d12h   100.64.130.222   ip-10-95-254-186.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-pmn4d   1/1     Running   0          2d14h   100.64.134.106   ip-10-95-254-183.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-st2lq   1/1     Running   0          2d13h   100.64.134.172   ip-10-95-254-183.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-t8z2k   1/1     Running   0          2d13h   100.64.131.13    ip-10-95-254-167.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-tf8cn   1/1     Running   0          2d13h   100.64.151.236   ip-10-95-255-176.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-tm4mk   1/1     Running   0          2d12h   100.64.149.36    ip-10-95-255-190.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-wtfl5   1/1     Running   0          2d13h   100.64.149.151   ip-10-95-255-190.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-xvn99   1/1     Running   0          2d14h   100.64.131.40    ip-10-95-254-186.ap-northeast-2.compute.internal   <none>           <none>
appr-api-prd-deploy-6974c449d9-zgz8w   1/1     Running   0          2d14h   100.64.149.61    ip-10-95-255-183.ap-northeast-2.compute.internal   <none>           <none>

#[cicdadm@ppmacmd1 alb-ingress]$ k top pod -n ubd
#NAME                                   CPU(cores)   MEMORY(bytes)
#appr-api-prd-deploy-6974c449d9-5m9br   48m          1000Mi


[cicdadm@ppmacmd1 alb-ingress]$ k top pod -n ubd
NAME                                   CPU(cores)   MEMORY(bytes)
appr-api-prd-deploy-6974c449d9-5m9br   48m          2573Mi
appr-api-prd-deploy-6974c449d9-5nwck   85m          2146Mi
appr-api-prd-deploy-6974c449d9-5qcxr   59m          2536Mi
appr-api-prd-deploy-6974c449d9-7kq5h   43m          2554Mi
appr-api-prd-deploy-6974c449d9-8knnm   63m          2541Mi
appr-api-prd-deploy-6974c449d9-9z7pl   46m          2548Mi
appr-api-prd-deploy-6974c449d9-ccmgs   36m          2567Mi
appr-api-prd-deploy-6974c449d9-dl5zz   66m          2522Mi
appr-api-prd-deploy-6974c449d9-dv66g   45m          2552Mi
appr-api-prd-deploy-6974c449d9-h2m28   38m          2555Mi
appr-api-prd-deploy-6974c449d9-lfgh9   53m          2554Mi
appr-api-prd-deploy-6974c449d9-nrd6n   39m          2533Mi
appr-api-prd-deploy-6974c449d9-pmn4d   45m          2579Mi
appr-api-prd-deploy-6974c449d9-st2lq   48m          2565Mi
appr-api-prd-deploy-6974c449d9-t8z2k   57m          2548Mi
appr-api-prd-deploy-6974c449d9-tf8cn   56m          2575Mi
appr-api-prd-deploy-6974c449d9-tm4mk   42m          2527Mi
appr-api-prd-deploy-6974c449d9-wtfl5   57m          2562Mi
appr-api-prd-deploy-6974c449d9-xvn99   47m          2557Mi
appr-api-prd-deploy-6974c449d9-zgz8w   44m          2554Mi
```
<br><br>


#### **- 성능테스트 - UBF 튜닝1**  

초기 4개의 Pod에서 부하를 시작하였을 때, 응답시간 지연이 쌓이기 시작하였다.  

![k04](/assets/it/project/kbcard/mydata/k04.png){: width="100%" height="auto"}  

Jennifer 상에서 증가하는 응답시간을 잡아보니 실제 SQL 수행시간이 오래걸렸고 해당 Query에 대한 튜닝을 진행하고 나서야 정상 응답시간으로 복구하였다.  

![k05](/assets/it/project/kbcard/mydata/k05.jpg){: width="100%" height="auto"}  

<br><br>



#### **- 성능테스트 - UBF 튜닝2**  

초기 4개의 Pod에서 부하를 시작하였을 때, Scale-Out이 되면서 갑자기 모든 Pod에 부하가 걸리면서 죽는 현상이 발생하였다.  
Scale-Out이 되었지만 Scale-Out 된 Pod로 아직은 부하가 들어오기 전이라는 시점이 이상하여 Scale-Out이 문제였는지, 아니면 다른게 이상했는지 판단하기가 어려웠다.  
그래서 다시 한 번 테스트 수행을 요청하였고, 이번에는 Scale-Out이 되기 전에 죽는현상이 있었다.  

따라서 기존 소스코드에 무엇이 문제였는지 파악이 시급했다.  
다시 한 번 Test를 돌리면서 죽을 때 쯔음 Thread Dump를 생성하기 시작했다.  

![k06](/assets/it/project/kbcard/mydata/k06.png){: width="100%" height="auto"}  

제니퍼 상에서는 DB Connection get failed 오류가 많이 발생하였고 (Timeout 30초), ThreadDump를 보니 JDBC Getconnection을 waiting 하는 Thread들이 많이 있었다.  

정확한 이유는 모르겠지만 Main Thread에서 Nested transaction으로 별도 Thread를 생성하여 로그를 추가로 Insert 하는 로직이 있었는데 해당 로직을 빼고 수행해보니  
Pod이 죽지 않고 정상적으로 수행하는 것을 확인하였다.  

Http요청하나가 종료할 때 까지 DB Connection Pool하나를 사용한다고하여 (추가확인필요) 모든 Connection Pool이 Http 요청을 처리하는 중  
Nested Transaction 으로 분기된 Thread에서 Getconnection을 할 때 오류가 발생하나 싶어, Nested Transcation용 Connection Pool을 별도로 생성하였으며  
소스 상에서 로그 Insert하는 Nested Transaction은 해당 Connection Pool에서 Connection을 가져다가 사용하도록 소스 변경을 하였고 정상 처리되었다.  

조금 더 정확한 사유 확인이 필요한 케이스이다.  

```bash
at sun.misc.Unsafe.park(Native Method) 
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215) 
at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037) 
at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328) 
at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277) 
at org.apache.tomcat.jdbc.pool.FairBlockingQueue.poll(FairBlockingQueue.java:153) 
at org.apache.tomcat.jdbc.pool.ConnectionPool.borrowConnection(ConnectionPool.java:686) 
at org.apache.tomcat.jdbc.pool.ConnectionPool.getConnection(ConnectionPool.java:198) 
at org.apache.tomcat.jdbc.pool.DataSourceProxy.getConnection(DataSourceProxy.java:132) 
at aries.runtime.tracer.JDBCConnectionTrace.getConnection(JDBCConnectionTrace.java:123) 
at aries.runtime.tracer.impl.ProfileSQLConnectionImpl.getConnection(ProfileSQLConnectionImpl.java:371) 
at aries.base.profile.ProfileSQL.getConnection(ProfileSQL.java:325) 
at aries.base.jdk.DataSource.getConnection(DataSource.java:76) 
at devonframework.persistent.connection.LJndiDataSource.getConnection(LJndiDataSource.java:130) 
at devonframework.persistent.connection.LDataSourcePool.getJNDIConnection(LDataSourcePool.java:342) 
at devonframework.persistent.connection.LDataSourcePool.getConnection(LDataSourcePool.java:137) 
at devonframework.persistent.dao.LConnectionManager.getConnection(LConnectionManager.java:86) 
at devonframework.business.transaction.LConnectionMapper.getConnection(LConnectionMapper.java:106) 
at devonframework.persistent.autodao.LAutoDao.getConnection(LAutoDao.java:239) 
at devonframework.persistent.autodao.LAutoDao.getConnection(LAutoDao.java:213) 
at devonframework.persistent.autodao.LCommonDao.executeQueryForSingle(LCommonDao.java:790) 
at devonframework.persistent.autodao.LCommonDao.executeQueryForSingle(LCommonDao.java:494) 
at devonenterprise.service.tranctrl.composite.CompositeTranCtrl.checkCompositeSvcCtrl(CompositeTranCtrl.java:86) 
at devonenterprise.ext.front.command.RestInboundCommand.execute(RestInboundCommand.java:307) 
at devonenterprise.ext.channel.interceptor.ExtCommandInterceptor.executeCommand(ExtCommandInterceptor.java:116) 
at devonenterprise.ext.channel.interceptor.ExtCommandInterceptor.execute(ExtCommandInterceptor.java:87) 
at devonenterprise.ext.channel.interceptor.ExtCommandInterceptor.doIntercept(ExtCommandInterceptor.java:45) 
at devonframework.front.channel.interceptor.LInterceptorChain.doIntercept(LInterceptorChain.java:93) 
at devonenterprise.ext.channel.interceptor.GuidCreateInterceptor.doIntercept(GuidCreateInterceptor.java:27) 
at devonframework.front.channel.interceptor.LInterceptorChain.doIntercept(LInterceptorChain.java:93) 
at devonframework.front.channel.interceptor.LConverterInterceptor.doIntercept(LConverterInterceptor.java:171) 
at devonframework.front.channel.interceptor.LInterceptorChain.doIntercept(LInterceptorChain.java:93) 
at devonframework.front.channel.LAbstractServlet.process(LAbstractServlet.java:290) 
at devonenterprise.ext.channel.adaptor.http.sync.impl.GeneralRestServlet.catchService(GeneralRestServlet.java:62) 
at devonframework.front.channel.LAbstractServlet.doCommon(LAbstractServlet.java:167) 
at devonframework.front.channel.LAbstractServlet.doPost(LAbstractServlet.java:106) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:652) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:733) 
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) 
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
```
<br><br>


#### **- 성능테스트 - UBF 튜닝3**  

위 의 이슈들이 해결되고, 정상적으로 부하를 받던 중 Scale-Out된 Pod이 정상적으로 Heatlh Check된 이 후 문제가 발생하였다.  
ALB Target에 등록되어 트래픽을 받을 때 응답시간의 지연이 발생하는 경우였다.  
Jennifer 상에서는 Queue에 Tomcat Max Thread 개수인 300개가 꽉차서 처리가 되는 것 처럼 보였고, 응답시간이 7초정도까지 찬 상태로 면을 그리듯이 응답시간이 생겼다.  

Auto Scale-Out 된 Pod에 부하가 천천히 들어왔으면 좋겠다는 생각을 했고, Target Group Attributes 속성의 Slow start duration 옵션을 확인하여 설정을 진행했다.  

> Slow start duration  
During this period, a newly registered target receives an increasing share of requests, until it reaches its fair share.
Requires 30 to 900 seconds to enable, or 0 seconds to disable. This attribute cannot be combined with the Least outstanding requests algorithm.

초기 설정은 120초로 진행했다가 60초로 변경하였으며 해당 값 설정 이 후 확연히 눈에띄게 Scale-Out 서비스가 안정적인 트래픽을 받기 시작했다.  
Scale-Out 시에도 응답의 지연은 전혀 없었으며 안정적인 처리가 모두 가능했다.  

<br><br>


#### **- 성능테스트 - UBF 튜닝4**  

장기부하 테스트를 진행 중 항상 8시간 ~ 9시간 정도의 성능테스트 후 어플리케이션에서 응답시간이 지연되는 현상이 생겼다.  
계속 유지가 되던 Pod의 수도 2개 정도 Scale-Out이 되는 현상이 발생했다. 이 현상은 장기부하테스트를 2~3번 진행하는 동안 매번 동일한 패턴으로 8시간 ~ 9시간 사이에 발생했다.  

아래 빨간색 박스를 친 부분이 바로 응답시간의 지연이 생기기 시작한 패턴이며 신규로 생성된 2개의 파드에서는 현상이 없었고, 기존 4개의 파드에서만 현상이 발생했다.  

![k09](/assets/it/project/kbcard/mydata/k09.png){: width="100%" height="auto"}  

<br>

현상이 이상하여 리소스 상태를 모니터링 해봤다. 그리고 이상한 점을 발견했다. 메모리의 상태를 모니터링 해보니 아래와 같은 현상이 발생하고 있었다.  
4초 이상 걸리는 GC들이 수 없이 많이 발생하고 있었다.  그리고 Old 영역의 메모리들이 전혀 줄어들고 있는 패턴을 보이지 않았다.   

![k11](/assets/it/project/kbcard/mydata/k11.png){: width="100%" height="auto"}  

<br>

조금 더 자세하게 보기 위해 Pod 내부로 접속을 했다. 
jstat 커맨드로 상황을 보니 5초에 한 번씩 status를 체크했는데 Full GC Count가 계속 올라가고 있었고, Old Used 메모리는 떨어지지 않았다.  
때문에 메모리 누수를 확신하고 HeapDump를 생성하여 분석하기로 하였다.  

![k12](/assets/it/project/kbcard/mydata/k12.png){: width="100%" height="auto"}  

`Command`
```bash
jstat -gc 1 5000 100
```

<br>

HeapDump를 생성하고 MAT(Memory Analyzer Tool)를 이용해 파일을 열었다. 때문에 메모리 누수를 확신하고 HeapDump를 생성하여 분석하기로 하였다.  

![k13](/assets/it/project/kbcard/mydata/k13.png){: width="100%" height="auto"}  

Overview만 봐도 벌써 어디선가 1GB의 Heap영역을 차지하고 있음을 알 수 있다. 그 프로그램이 무엇인지를 찾기 위해 Dominator_tree를 열었다.

<br>

Retained_Heap 영역이 큰 것부터 찾아나가기 시작했다. 확률적으로 Retained_Heap이 큰 것 부터 찾으면 비정상적으로 금방 찾을 확률이 높다.  
Retained_Heap 메모리가 큰 것은 누수가 있기 때문에 지속적으로 참조메모리를 생성하고 있기 때문에 얼추 맞다.  

![k14](/assets/it/project/kbcard/mydata/k14.png){: width="100%" height="auto"}  

모든 Thread의 참조 메모리가 컸다. http-nio-11010-exec-2 번 Thread를 열어보기 시작했다.  

<br>

![k15](/assets/it/project/kbcard/mydata/k15.png){: width="100%" height="auto"}  

java.lang.Object 하위로 MapHashMap이 어디선가 5만개 가까이 생성 된 후 남아있었다.  

<br>

![k16](/assets/it/project/kbcard/mydata/k16.png){: width="100%" height="auto"}  

Framework 어디선가 Trace.log에 대한 HashMap을 생성하고 지속적으로 참조를 하고 있었기 때문에 HashMap 객체가 생성 된 후, 메모리 해제가 안되었던 현상이었다.   

<br>


![k17](/assets/it/project/kbcard/mydata/k17.png){: width="100%" height="auto"}  

Framework에서 트랜잭션이 끝난 후, resetAll() 호출을 통해 HashMap 변수에 null 처리를 해주도록 했으며 값이 없다보니 GC 시에 정상 메모리 해제가 된 듯 보였다. 
조치 후에 생긴 메모리는 위와 같이 변했다.  

<br>
